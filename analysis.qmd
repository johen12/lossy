---
title: Memory and expectation under the same roof
subtitle: Plots and analysis
echo: false
format:
    pdf:
        documentclass: article
        margin-left: 20mm
        margin-right: 20mm
---

```{python}
#| label: setup
#| echo: false

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from pytensor import function
import pytensor.tensor as pt
import expdata
from matplotlib.colors import CenteredNorm

from lossy_tensor import ProgressiveNoiseModel
from grammars import *

sns.set_theme()

def gen_diffs(sequence1: list[str], 
              sequence2: list[str],
              model: ProgressiveNoiseModel,
              max_retention_probabilities: np.array,
              rate_falloffs: np.array,
              max_depth = None):
    diffs = np.zeros(
        (len(max_retention_probabilities), len(rate_falloffs))
    )
    delta = pt.scalar("delta")
    nu = pt.scalar("nu")
    processing_difficulty = function(
        [delta, nu],
        model.processing_difficulty([sequence1, sequence2], delta, nu)
    )
    for (i, max_rp) in enumerate(max_retention_probabilities):
        for (j, rf) in enumerate(rate_falloffs):
            [D_seq1, D_seq2] = processing_difficulty(max_rp, rf)
            diffs[i, j] = D_seq1 - D_seq2
            if np.isnan(diffs[i, j]):
                print(max_rp, rf)

    return diffs

def plot_diff_heatmap(
    sequence1: list[str],
    sequence2: list[str],
    model: ProgressiveNoiseModel,
    axis,
    step = 0.01,
    split = 5,
    shrink = 0.7,
    render_label = True,
    label = "Difference in processing difficulty (bits)",
    **kwargs
):
    deltas = np.arange(0, 1.0 + step, step)
    nus = np.arange(0, 1.0 + step, step)

    diffs = gen_diffs(sequence1, sequence2, model, deltas, nus)

    yticklabels = [f"{val:.2f}" if i % split == 0 else "" for (i, val) in enumerate(deltas)]
    xticklabels = [f"{val:.2f}" if i % split == 0 else "" for (i, val) in enumerate(nus)]

    norm = CenteredNorm(vcenter = 0)

    cbar_kws = dict(shrink = shrink)
    if render_label:
        cbar_kws["label"] = label

    sns.heatmap(
        diffs,
        xticklabels = xticklabels,
        yticklabels = yticklabels,
        cmap = "coolwarm",
        norm = norm,
        rasterized = True,
        square = True,
        cbar_kws = cbar_kws,
        ax = axis,
        **kwargs
    ).set(
        xlabel = "$\\nu$",
        ylabel = "$\\delta$"
    )

    axis.tick_params("x", rotation = 90)
    axis.tick_params("y", rotation = 0)

model_russian = ProgressiveNoiseModel(pcfg_russian, 0, 0)
model_hindi = ProgressiveNoiseModel(pcfg_cpsp_hindi, 0, 0)
model_persian = ProgressiveNoiseModel(pcfg_cpsp_persian, 0, 0)
```

# Russian experiment
## Grammar

| **Rule**                           | **Probability**                                                    |
| ---------------------------------- | ------------------------------------------------------------------ |
| RC $\to$ SRC                       | $p(\text{SRC})$                                                    |
| RC $\to$ ORC                       | $1-p(\text{SRC})$                                                  |
| SRC $\to$ SRCRP 'V' ArgSRC         | $p(\text{SRC Local})(1-p(\text{Adjunct intervener}))$              |
| SRC $\to$ SRCRP ArgSRC 'V'         | $(1-p(\text{SRC Local}))(1-p(\text{Adjunct intervener}))$          |
| SRC $\to$ SRCRP AdjIntv 'V' ArgSRC | $p(\text{Adjunct intervener})p(\text{SRC Local})$                  |
| SRC $\to$ SRCRP AdjIntv ArgSRC 'V' | $p(\text{Adjunct intervener})(1-p(\text{SRC Local}))$              |
| SRCRP $\to$ 'RPNom'                | $p(\text{SRC Case marked})$                                        |
| SRCRP $\to$ 'chto'                 | $1-p(\text{SRC Case marked})$                                      |
| ArgSRC $\to$ 'DO'                  | $p(\text{One argument})$                                           |
| ArgSRC $\to$ 'DO' 'IO'             | $1-p(\text{One argument})$                                         |
| ORC $\to$ ORCRP 'V' ArgORC         | $p(\text{ORC Local})(1-p(\text{Adjunct intervener}))$              |
| ORC $\to$ ORCRP ArgORC 'V'         | $(1-p(\text{ORC Local}))(1-p(\text{Adjunct intervener}))$          |
| ORC $\to$ ORCRP AdjIntv 'V' ArgORC | $p(\text{Adjunct intervener})p(\text{ORC Local})$                  |
| ORC $\to$ ORCRP AdjIntv ArgORC 'V' | $p(\text{Adjunct intervener})(1-p(\text{ORC Local}))$              |
| ORCRP $\to$ 'RPAcc'                | $p(\text{ORC Case marked})$                                        |
| ORCRP $\to$ 'chto'                 | $1-p(\text{ORC Case marked})$                                      |
| ArgORC $\to$ 'Subj'                | $p\left( \text{One argument} \right)$                              |
| ArgORC $\to$ 'Subj' 'IO'           | $1-p\left( \text{One argument} \right)$                            |
| AdjIntv $\to$ 'Adj1'               | $p(\text{One adjunct})\cdot 0.5$                                   |
| AdjIntv $\to$ 'Adj2'               | $p(\text{One adjunct})\cdot 0.5$                                   |
| AdjIntv $\to$ 'Adj1' 'Adj2'        | $0.5(1-p(\text{One adjunct}))$                                     |
| AdjIntv $\to$ 'Adj2' 'Adj1'[^note] | $0.5(1-p(\text{One adjunct}))$                                     |

: The PCFG used to model the Russian results. Terminal symbols are surrounded by single quotes. {#tbl-rus-pcfg} {tbl-colwidths="[50, 50]"}

[^note]: The use of two unique adjunct symbols is a consequence of the implementation of the progressive noise model. Using the same symbol for both adjuncts makes it impossible to discern if the first or the second adjunct has been deleted, and since the retention probability is dependent on the position of the word, the distortion probability becomes impossible to calculate correctly.

### Probabilities
The probabilities were calculated as in @tbl-rus-probs.

| **Probability**             | **Expression**                                                                                                                  | **Calculation**                            | **Value[^2]** |
| --------------------------- | ------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------ | --------------------------------- |
| $p(\text{SRC})$             | $\frac{\text{total number of SRCs}}{\text{total number of RCs}}$                                                                | $\frac{154+9+17+2}{154+9+17+2+42+74+9+14}$ | $0.57$                            |
| $p(\text{SRC local})$       | $\frac{\text{total number of local SRCs}}{\text{total number of SRCs}}$                                                         | $\frac{154+17}{154+17+9+2}$                | $0.94$                            |
| $p(\text{SRC case-marked})$ | $\frac{\text{total number of case-marked SRCs}}{\text{total number of SRCs}}$                                                   | $\frac{154+9}{154+9+17+2}$                 | $0.9$                             |
| $p(\text{ORC local})$       | $\frac{\text{total number of local ORCs}}{\text{total number of ORCs}}$                                                         | $\frac{42+9}{52+9+74+14}$                  | $0.37$                            |
| $p(\text{ORC case marked})$ | $\frac{\text{total number of case-marked ORCs}}{\text{total number of ORCs}}$                                                   | $\frac{42+74}{42+74+9+14}$                 | $0.83$                            |
| $p(\text{AdjIntv})$         | $\frac{\text{at least one adjunct intervener}}{\text{any number of interveners}}$                                               | $\frac{925}{5851}$                         | $0.16$                            |
| $p(\text{One adjunct})$     | $\frac{\text{at least one adjunct intervener}-\text{at least two adjunct interveners}}{\text{at least one adjunct intervener}}$ | $\frac{925-49}{925}$                       | $0.95$                            |
| $p(\text{One argument})$    | $\frac{\text{at least one argument}-\text{at least two arguments}}{\text{at least one argument}}$                               | $\frac{1621-107}{1621}$                    | $0.93$                            |

: Probabilities used in the Russian PCFG. The first five were gathered from Table 1 of Levy (p. 467) and the last three using the PML Tree Query Engine (see section Queries below). {#tbl-rus-probs} {tbl-colwidths="[20, 50, 23, 7]"}

[^2]: Rounded to two digits.

#### Counts


#### Queries
*Any number of interveners:*
```
a-node $v := [
  tag="VERB",
  deprel = "acl:relcl",
  child a-node $r := [
    tag = "PRON",
    lemma = "который"
  ],
] >> count()
```

*At least one adjunct intervener:*
```
a-node $v := [
  tag="VERB",
  deprel = "acl:relcl",
  child a-node $r := [
    tag = "PRON",
    lemma = "который",
  ],
  
  child a-node [
    tag = "ADV",
    deprel = "advmod",
    order-follows $r,
    order-precedes $v
  ]
] >> count()
```

*At least two adjunct interveners:*
```
a-node $v := [
  tag="VERB",
  deprel = "acl:relcl",
  child a-node $r := [
    tag = "PRON",
    lemma = "который",
  ],
  
  child a-node [
    tag = "ADV",
    deprel = "advmod",
    order-follows $r,
    order-precedes $v
  ],
  
  child a-node [
    tag = "ADV",
    deprel = "advmod",
    order-follows $r,
    order-precedes $v
  ],
] >> count()
```

*At least one argument:*
```
a-node $v := [
  tag="VERB",
  deprel = "acl:relcl",
  child a-node $r := [
    tag = "PRON",
    lemma = "который",
    deprel = "nsubj" or deprel = "obj"
  ],
  
  child a-node [
    deprel = "obj" or deprel = "nsubj",
  ],
  
  child a-node [
    deprel = "iobj"
  ]
] >> count()
```

*At least two arguments:*
```
a-node $v := [
  tag="VERB",
  deprel = "acl:relcl",
  child a-node $r := [
    tag = "PRON",
    lemma = "который",
    deprel = "nsubj" or deprel = "obj"
  ],
  
  child a-node [
    deprel = "obj" or deprel = "nsubj",
  ],
  
  child a-node [
    deprel = "iobj"
  ]
] >> count()
```

## Results
### Experiment 1
#### Experiment 1a
In Experiment 1a, subject- and object-extracted relative clauses were investigated, with the main manipulation being the placement of the verb (directly following the )

```{python}
#| label: fig-rus-exp1a-heatmap-verb
#| fig-pos: H
#| fig-cap: "Difference in processing difficulty at the verb between non-local and local SRCs (a) and ORCs (b), respectively. Red indicates locality and blue anti-locality."
#| layout-ncol: 2
#| fig-subcap:
#|   - "Subject-extracted relative clauses."
#|   - "Object-extracted relative clauses."

plot_diff_heatmap("RPNom DO V".split(), "RPNom V".split(), model_russian, plt.gca())
plt.show();
plot_diff_heatmap("RPAcc Subj V".split(), "RPAcc V".split(), model_russian, plt.gca())
plt.show();
```

```{python}
#| label: fig-rus-exp1a-heatmap-acc
#| fig-pos: H
#| fig-cap: "Difference in processing difficulty at the accusative NP placed postverbally and preverbally. Blue indicates a surprisal effect, in line with the empirical results."

plot_diff_heatmap("RPNom V DO".split(), "RPNom DO".split(), model_russian, plt.gca())
plt.show();
```

#### Experiment 1b
```{python}
#| label: fig-rus-exp1b-heatmap-verb
#| fig-pos: H
#| fig-cap: "Difference in processing difficulty at the verb between non-local and local SRCs (a) and ORCs (b) with the case-synchretized relative pronoun 'chto', respectively. Red indicates locality and blue anti-locality."
#| layout-ncol: 2
#| fig-subcap:
#|   - "Case-synchretized subject-extracted relative clauses."
#|   - "Case-synchretized object-extracted relative clauses."

plot_diff_heatmap("chto DO V".split(), "chto V".split(), model_russian, plt.gca())
plt.show();
plot_diff_heatmap("chto Subj V".split(), "chto V".split(), model_russian, plt.gca())
plt.show();
```

### Experiment 2
```{python}
#| label: fig-rus-exp2-heatmap-verb
#| fig-pos: H
#| fig-cap: "Difference in predicted processing difficulty at the verb between Experiment 2 conditions. Red indicates locality and blue anti-locality. Note also the much smaller scales of the effect of adjunct interveners."
#| fig-subcap:
#|   - "Argument interveners."
#|   - "Adjunct interveners."

split = 20
shrink = 0.35

fig_arg, axes_arg = plt.subplots(1, 3, sharex = True, sharey = True)

plot_diff_heatmap("RPNom DO V".split(),
                  "RPNom V".split(),
                  model_russian,
                  axes_arg[0],
                  split = split,
                  shrink = shrink,
                  render_label = False)
axes_arg[0].set_title("1 Arg - 0 Intv")

plot_diff_heatmap("RPNom DO IO V".split(),
                  "RPNom DO V".split(),
                  model_russian,
                  axes_arg[1],
                  split = split,
                  shrink = shrink,
                  render_label = False)
axes_arg[1].set_title("2 Args - 1 Arg")
axes_arg[1].set_ylabel("")

plot_diff_heatmap("RPNom DO IO V".split(),
                  "RPNom V".split(),
                  model_russian,
                  axes_arg[2],
                  split = split,
                  shrink = shrink,
                  label = "Difference (bits)")
axes_arg[2].set_title("2 Args - 0 Intv")
axes_arg[2].set_ylabel("")

plt.subplots_adjust(wspace = 0.5)

plt.show();

fig_adj, axes_adj = plt.subplots(1, 3, sharex = True, sharey = True)

plot_diff_heatmap("RPNom Adj1 V".split(),
                  "RPNom V".split(),
                  model_russian,
                  axes_adj[0],
                  split = split,
                  shrink = shrink,
                  render_label = False)
axes_adj[0].set_title("1 Adj - 0 Intv")

plot_diff_heatmap("RPNom Adj1 Adj2 V".split(),
                  "RPNom Adj1 V".split(),
                  model_russian,
                  axes_adj[1],
                  split = split,
                  shrink = shrink,
                  render_label = False)
axes_adj[1].set_title("2 Adjs - 1 Adj")
axes_adj[1].set_ylabel("")

plot_diff_heatmap("RPNom Adj1 Adj2 V".split(),
                  "RPNom V".split(),
                  model_russian,
                  axes_adj[2],
                  split = split,
                  shrink = shrink,
                  label = "Difference (bits)")
axes_adj[2].set_title("2 Adjs - 0 Intv")
axes_adj[2].set_ylabel("")

plt.subplots_adjust(wspace = 0.5)

plt.show();
```

# Persian and Hindi
## Grammar
| **Rule**                          | **Probability**                           |
| --------------------------------- | ----------------------------------------- |
| S $\to$ CPP                       | $p(\text{CP})$                            |
| CPP $\to$ 'CPNoun' CPIntv CPVerb  | $p(\text{CP Intervener})$                 |
| CPP $\to$ 'CPNoun' CPVerb         | $1-p(\text{CP Intervener})$               |
| CPIntv $\to$ 'Adj1'               | $0.5\cdot p(\text{CP Short})$             |
| CPIntv $\to$ 'Adj2'               | $0.5\cdot p(\text{CP Short})$             |
| CPIntv $\to$ 'Adj1' 'Adj2'        | $0.5(1-p(\text{CP Short}))$               |
| CPIntv $\to$ 'Adj2' 'Adj1'        | $0.5(1-p(\text{CP Short}))$               |
| CPVerb $\to$ 'LightVerb'          | $p(\text{Light verb}\mid \text{CP})$      |
| CPVerb $\to$ 'OtherVerb'          | $1-p(\text{Light verb}\mid \text{CP})$    |
| S $\to$ SPP                       | $1-p(\text{CP})$                          |
| SPP $\to$ 'SPNoun' SPIntv SPVerb  | $p(\text{SP Intervener})$                 |
| SPP $\to$ 'SPNoun' SPVerb         | $1-p(\text{SP Intervener})$               |
| SPIntv $\to$ 'Adj1'               | $0.5\cdot p(\text{SP Short})$             |
| SPIntv $\to$ 'Adj2'               | $0.5\cdot p(\text{SP Short})$             |
| SPIntv $\to$ 'Adj1' 'Adj2'        | $0.5(1-p(\text{SP Short}))$               |
| SPIntv $\to$ 'Adj2' 'Adj1'        | $0.5(1-p(\text{SP Short}))$               |
| SPVerb $\to$ 'LightVerb'          | $p(\text{Light verb}\mid \text{SP})$      |
| SPVerb $\to$ 'OtherVerb'          | $1-p(\text{Light verb}\mid \text{SP})$    |

: The PCFG used to model complex and simple predicates in Persian and Hindi. {#tbl-cpsp-pcfg} {tbl-colwidths="[50, 50]"}

### Probabilities
|                                      | **Description**                                                                                                                                 | **Hindi**                            | **Persian**                           |
| ------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------ | ------------------------------------- |
| $p(\text{CP})$                       | the probability of the sentence containing a complex predicate.                                                                                 | $`{python} hindi_p_cp`$              | $`{python} persian_p_cp`$             |
| $p(\text{CP Intervener})$            | the probability of a complex predicate construction having intervening adjuncts.                                                                | $`{python} hindi_p_cp_intv`$         | $`{python} persian_p_cp_intv`$        |
| $p(\text{CP Short})$                 | the probability of a complex predicate having only one intervener between the noun and the light verb.                                          | $`{python} hindi_p_cp_short`$        | $`{python} persian_p_cp_short`$       |
| $p(\text{Light verb}\mid \text{CP})$ | the probability of the specific light verb appearing, given that the noun in the sentence was the corresponding complex predicate noun.         | $`{python} hindi_p_cp_lightverb`$    | $`{python} persian_p_cp_lightverb`$   |
| $p(\text{SP Intervener})$            | the probability of a simple predicate construction having intervening adjuncts.                                                                 | $`{python} hindi_p_sp_intv`$         | $`{python} persian_p_sp_intv`$        |
| $p(\text{SP Short})$                 | the probability of a simple predicate having only one intervener between the noun and the light verb.                                           | $`{python} hindi_p_sp_short`$        | $`{python} persian_p_sp_short`$       |
| $p(\text{Light verb}\mid \text{SP})$ | the probability of a the specific light verb appearing, given that the noun in the sentence was *not* the corresponding complex predicate noun. | $`{python} hindi_p_sp_lightverb`$    | $`{python} persian_p_sp_lightverb`$   |

: The probabilities used for the Hindi/Persian grammar. {#tbl-cpsp-probs} {tbl-colwidths="[23, 57, 10, 10]"}

The probabilities of light verbs in the different sentence types were taken from sentence completion studies conducted by @husainetal2014 and @safavietal2016, respectively. The rest were calculated according to @tbl-cpsp-prob-calc.

| **Probability**           | **Expression**                                                                                               |
| ------------------------- | ------------------------------------------------------------------------------------------------------------ |
| $p(\text{CP})$            | $\frac{\text{CPs}}{\text{CPs+SPs}}$                                                                          |
| $p(\text{SP intervener})$ | $\frac{\text{SPs at least one intervener}}{\text{SPs}}$                                                      |
| $p(\text{SP Short})$      | $\frac{\text{SPs at least one intervener-SPs at least two interveners}}{\text{SPs at least one intervener}}$ |
| $p(\text{CP intervener})$ | $\frac{\text{CPs at least one intervener}}{\text{CPs}}$                                                      |
| $p(\text{CP Short})$      | $\frac{\text{CPs at least one intervener-CPs at least two interveners}}{\text{CPs at least one intervener}}$ |

: Expressions for calculating probabilities for the Persian/Hindi PCFGs. {#tbl-cpsp-prob-calc} {tbl-colwidths="[25, 75]"}

#### Counts
##### Hindi
|                      | HDTB | PUD | Total |
| -------------------- | ---- | --- | ----- |
| SP                   | 2291 | 493 | 2784  |
| CP                   | 2808 | 2   | 2810  |
| SP at least one intv | 122  | 40  | 162   |
| SP at least two intv | 1    | 0   | 1     |
| CP at least one intv | 138  | 2   | 140   |
| CP at least two intv | 0    | 0   | 0     |

##### Persian
|                      | PerDT | Seraji | Total |
| -------------------- | ----- | ------ | ----- |
| SP                   | 3748  | 505    | 4253  |
| CP                   | 9597  | 1108   | 10705 |
| SP at least one intv | 80    | 17     | 97    |
| SP at least two intv | 1     | 0      | 1     |
| CP at least one intv | 2     | 0      | 2     |
| CP at least two intv | 0     | 0      | 0     |

#### Queries
The following queries work for Persian. Because of labeling differences between Hindi and Persian, `compound:lvc` has to be changed to just `compound` for the queries to work for the Hindi corpora.

##### Simple predicates
*Any number of interveners*
```
a-node $v := [
	tag = "VERB",
	deprel = "root",
	child a-node [
		deprel = "obj",
		tag = "NOUN"
	],
	!child a-node [
	  deprel = "compound:lvc",
	]
] >> count()
```

*With at least one intervener:*
```
a-node $v := [
	tag = "VERB",
	deprel = "root",
	child a-node $o := [
		deprel = "obj",
		tag = "NOUN"
	],
	!child a-node [
	  deprel = "compound:lvc",
	],
	child a-node [
		order-precedes $v,
		order-follows $o,
		deprel = "advmod"
	]
] >> count()
```

*With two interveners:*
```
a-node $v := [
	tag = "VERB",
	deprel = "root",
	child a-node $o := [
		deprel = "obj",
		tag = "NOUN"
	],
	!child a-node [
	  deprel = "compound:lvc",
	],
	child a-node [
		order-precedes $v,
		order-follows $o,
		deprel = "advmod"
	],
	child a-node [
		order-precedes $v,
		order-follows $o,
		deprel = "advmod"
	]
] >> count()
```

##### Complex predicates
*Any number of interveners:*
```
a-node $v := [
	tag = "VERB",
	deprel = "root",
	!child a-node [
		deprel = "obj",
	],
	child a-node [
	  deprel = "compound:lvc",
	  tag = "NOUN"
	]
] >> count()
```

*With one intervener:*
```
a-node $v := [
	tag = "VERB",
	deprel = "root",
	!child a-node [
		deprel = "obj",
	],
	child a-node $o := [
	  deprel = "compound:lvc",
	  tag = "NOUN"
	],
	child a-node [
		order-precedes $v,
		order-follows $o,
		deprel = "advmod"
	]
] >> count()
```

*With two interveners:*
```
a-node $v := [
	tag = "VERB",
	deprel = "root",
	!child a-node [
		deprel = "obj",
	],
	child a-node $o := [
	  deprel = "compound:lvc",
	  tag = "NOUN"
	],
	child a-node [
		order-precedes $v,
		order-follows $o,
		deprel = "advmod"
	],
	child a-node [
		order-precedes $v,
		order-follows $o,
		deprel = "advmod"
	]
] >> count()
```

## Results
```{python}
#| label: fig-cpsp
#| fig-pos: H
#| fig-cap: "Difference in predicted processing difficulty at the light verb between long and short conditions for CPs (left) and SPs (right)."
#| fig-subcap:
#|  - "Hindi."
#|  - "Persian."

split = 20
shrink = 0.4

fig, axes = plt.subplots(1, 2, sharex = True, sharey = True, layout = "tight")

plot_diff_heatmap("CPNoun Adj1 Adj2 LightVerb".split(),
                  "CPNoun Adj1 LightVerb".split(),
                  model_hindi,
                  axes[0],
                  split = split,
                  shrink = shrink,
                  render_label = False)

plot_diff_heatmap("SPNoun Adj1 Adj2 LightVerb".split(),
                  "SPNoun Adj1 LightVerb".split(),
                  model_hindi,
                  axes[1],
                  split = split,
                  shrink = shrink,
                  render_label = False)

plt.show();

fig, axes = plt.subplots(1, 2, sharex = True, sharey = True, layout = "tight")

plot_diff_heatmap("CPNoun Adj1 Adj2 LightVerb".split(),
                  "CPNoun Adj1 LightVerb".split(),
                  model_persian,
                  axes[0],
                  split = split,
                  shrink = shrink,
                  render_label = False)

plot_diff_heatmap("SPNoun Adj1 Adj2 LightVerb".split(),
                  "SPNoun Adj1 LightVerb".split(),
                  model_persian,
                  axes[1],
                  split = split,
                  shrink = shrink,
                  render_label = False)

plt.show();
```